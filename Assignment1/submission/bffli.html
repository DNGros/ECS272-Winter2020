<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <h2>ECS 272 Assignment 1: Universal adversarial perturbations</h1>
    <h3>Xuerui Li</h3>
    <h3>SID: 916641540</h3>
    <style>
      /* Use css to modify the style of your HTML page
      *  where the visualization should be horizontally
      *  aligned to the middle of the page.
      */
      .page-content {
        padding: 1em;
      }
      .imag{
        width: auto;
        height: auto;
        max-width: 10%;
        max-height: 10%;
      }
    </style>
  </head>
  <body>
    <div class="page-content"></div>
    <div align="center">
      <img src="https://d3i71xaburhd42.cloudfront.net/16aa01ca0834a924c25faad5d8bfef3fd1acfcfe/1-Figure1-1.png" />
      <p>
      Image source: <a href="https://arxiv.org/pdf/1610.08401.pdf">Universal adversarial perturbations</a>
      </p>
    </div>

    <div class="text">
        <p>For arbitrary given inputs, this model can add a particular perturbation that can cause misclassifying. This perturbation is universal, namely the perturbation is only determined by the model itself, instead of the inputs. The images on the left are original images, while those on the right are perturbed images. The labels are shown above the arrows. It's static. This visualization clearly shows how the labels change after adding the perturbation, and leverage grids and arrows to make it looks better. It concisely illustrate the main findings in this paper.</p>
    </div>
  </div>
  </body>
</html>
